{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Productionization: APIs and Docker\n",
    "\n",
    "While a data scientist might rely heavily on Jupyter notebooks to run local experiments\n",
    "and choose a model, the story does not end here. Ultimately the goal is to get a model\n",
    "into _production_, i.e., make it available for use by the data scientist's clients, whether\n",
    "they be internal or external clients. Some of the components that go into this\n",
    "include\n",
    "\n",
    "1. Exposing the model via an API\n",
    "2. Standardizing the build and production environments\n",
    "3. Monitoring model performance\n",
    "4. Logging diagnostic events / troubleshooting errors\n",
    "\n",
    "We will focus on the first two points above.\n",
    "\n",
    "## Exposing a model via API\n",
    "\n",
    "API stands for _Application Programming Interface_ and refers to how your\n",
    "code is exposed to others. For example, people will talk about the fact\n",
    "that `matplotlib` as two APIs: the one based on repeatedly plotting and \n",
    "the one based on using `axes` objects. You also hear people talk about accessing\n",
    "Twitter's API to get the content of tweets.\n",
    "\n",
    "For our case, we will mean making a model available for others to call.\n",
    "\n",
    "### Building a model\n",
    "\n",
    "Before jumping into exposing a model, let's build one first. Since the\n",
    "modelling isn't the point here, we'll just build a gradient boosted machine\n",
    "predictor to predict on the classicial \n",
    "[Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "30                 4.8               3.1                1.6               0.2   \n",
       "74                 6.4               2.9                4.3               1.3   \n",
       "29                 4.7               3.2                1.6               0.2   \n",
       "135                7.7               3.0                6.1               2.3   \n",
       "\n",
       "     target  \n",
       "25        0  \n",
       "30        0  \n",
       "74        1  \n",
       "29        0  \n",
       "135       2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# load dataset\n",
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "df_iris['target'] = iris['target']\n",
    "\n",
    "# create train test split\n",
    "df_train, df_test = train_test_split(df_iris,\n",
    "                                     test_size=0.3,\n",
    "                                     random_state=47,\n",
    "                                     stratify=df_iris['target'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of fit model: 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# put together pipeline\n",
    "model = Pipeline([\n",
    "    ('scale', StandardScaler())\n",
    "    ,('gbm', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# make model\n",
    "model.fit(df_train[iris['feature_names']], df_train['target'])\n",
    "\n",
    "# see how accurate it is on test data\n",
    "test_predictions = model.predict(df_test[iris['feature_names']])\n",
    "test_accuracy = accuracy_score(df_test['target'], test_predictions)\n",
    "print('Test accuracy of fit model: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to deliver this model would be to actually deliver this\n",
    "notebook to a user, but that requires that the user be able to run a Jupyter\n",
    "notebook and know how to call the model. This might be a valid way to deliver\n",
    "a model to an internal user who is technically proficient such as a data\n",
    "analyst or another data scientist.\n",
    "But instead of delivering a notebook, we can binarize the model object itself\n",
    "using the `pickle` module which is built into base python and save it to disk. So let's do this\n",
    "and save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be reloaded from disk and called as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model_ = pickle.load(f)\n",
    "\n",
    "print('model prediction:', model_.predict(np.random.normal(size=(1, 4)))[0])\n",
    "\n",
    "del model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model saved as a pickle file, you can then pass the model easily between different users\n",
    "without them having to retrain the model. This isn't really a way to deliver a model to an\n",
    "end user, but again, more of a way to pass models around between internal users.\n",
    "\n",
    "### Flask\n",
    "\n",
    "Now let's move on to exposing the model via an API. We will be using\n",
    "[Flask](https://flask.palletsprojects.com/en/1.1.x/), which\n",
    "is one of several web micro-fameworks for python. A _web framework_ is\n",
    "a set of tools for a language that helps programmers deal with common tasks\n",
    "necessary to set up web applications. This can include standardizing names\n",
    "and interactions with databases, serving static content such as images from \n",
    "specific file directory locations, and allowing language specific code to be\n",
    "used in setting up the HTML files that are ultimately displayed in a public\n",
    "facing website. Examples of framworks include \n",
    "[django](https://www.djangoproject.com/) and\n",
    "[cherryPy](https://cherrypy.org/) for python and \n",
    "[rails](https://rubyonrails.org/) for ruby. Flask is fairly lightweight and \n",
    "easy to get started with, so we will use this.\n",
    "\n",
    "A simple application might look like the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing application.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile application.py\n",
    "\n",
    "# imports related to the model we have built\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# imports related to flask and loading the model\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "\n",
    "json_header = {'content-type': 'application/json; charset=UTF-8'}\n",
    "model = pickle.load(open('model.pickle', 'rb'))\n",
    "app = Flask(__name__)    \n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_prediction():\n",
    "    \n",
    "    args = request.args\n",
    "\n",
    "    # check that all args are present\n",
    "    desired_args = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    missing_args = [a for a in desired_args if args.get(a) is None]\n",
    "    \n",
    "    if len(missing_args) > 0:\n",
    "        error_msg = 'argument(s) missing: {}'.format(missing_args)\n",
    "        return (jsonify(error_msg), 422, json_header)\n",
    "                \n",
    "    # check that all args are floats\n",
    "    def arg_is_float(arg):\n",
    "        is_float = False\n",
    "        \n",
    "        try:\n",
    "            x = float(arg)\n",
    "            is_float = True\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        return is_float\n",
    "    \n",
    "    nonfloat_args = [a for a in desired_args if not arg_is_float(args.get(a))]\n",
    "    \n",
    "    if len(nonfloat_args) > 0:\n",
    "        error_msg = 'argument(s) not float: {}'.format(nonfloat_args)\n",
    "        return (jsonify(error_msg), 422, json_header)\n",
    "    \n",
    "    # make predictions\n",
    "    X = [[float(args.get(a)) for a in desired_args]]\n",
    "    prediction = str(model.predict(X)[0])\n",
    "    \n",
    "    return (jsonify({'prediction': prediction}), 200, json_header)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now written out to the file `application.py`. Without getting very technical, \n",
    "we'll point out some of the important features.\n",
    "\n",
    "At the top of the file we have\n",
    "```python\n",
    "json_header = {'content-type': 'application/json; charset=UTF-8'}\n",
    "model = pickle.load('model.pickle')\n",
    "app = Flask(__name__)    \n",
    "```\n",
    "Here we have a JSON header that is a `dict`, we load the model from a \n",
    "pickle file, and we create a `Flask` object using the `__name__` variable.\n",
    "The JSON header is used to tell the user calling our application what\n",
    "sort of data to expect to receive in response to the request they make.\n",
    "\n",
    "Note that at the bottom of the file we have\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0')\n",
    "```\n",
    "This causes the application to run with host `0.0.0.0`, aka the `localhost` aka\n",
    "our local machine will run the application.\n",
    "\n",
    "Between the initial creation of the application and\n",
    "running it if appropriate we have the following lines.\n",
    "```python\n",
    "@app.route('/', methods=['GET'])\n",
    "def get_prediction():\n",
    "    \n",
    "    args = request.args\n",
    "```\n",
    "Here we have a function named `get_prediction()` that is decorated with \n",
    "`@app.route('/', methods=['GET'])`. This tells Flask that to look for an HTTP `GET` request\n",
    "at the location `/`. If the application receives a request at this location, it\n",
    "then tries to run this function.  The first thing that happens in this function\n",
    "is to store `request.args` as `args`. Note that `request` is not mentioned anywhere\n",
    "in the module as either a local or global variable. This means that `request` must\n",
    "be a parameter that is inherited from the `@app.route` decoration. Indeed it is, and it\n",
    "contains all the information the application receives from the user at this route.\n",
    "The function then processes `args`, which is a `dict`. In a minute we will see some of\n",
    "what gets put into this variable.\n",
    "\n",
    "Following this is code that attempts to validates the contents of `args`. We\n",
    "first check that all of our desired arguments are included among the keys of the\n",
    "incoming arguments.\n",
    "```python\n",
    "    desired_args = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    missing_args = [a for a in desired_args if args.get(a) is None]\n",
    "    \n",
    "    if len(missing_args) > 0:\n",
    "        error_msg = 'argument(s) missing: {}'.format(missing_args)\n",
    "        return (jsonify.dumps(error_msg), 422, json_header)\n",
    "```\n",
    "Here we see that if some of the desired arguments are missing we exit\n",
    "the function early. We dump an error message as a JSON object\n",
    "along with the [_HTTP response status code_](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) \n",
    "422, which indicates that the application was not able to process the request properly.\n",
    "We also pass in our json header object. Similarly, we then check that the values of the\n",
    "arguments can be cast as floats.\n",
    "\n",
    "The function ends by making a prediction and writing out the result with response status\n",
    "code 200, the everything was fine on our end signal.\n",
    "```python\n",
    "    X = [[float(args.get(a)) for a in desired_args]]\n",
    "    prediction = model.predict(X)[0]\n",
    "    \n",
    "    return (jsonify({'prediction': prediction}), 200, json_header)\n",
    "```\n",
    "\n",
    "Then, in your terminal, you can start the application locally by running\n",
    "```bash\n",
    "$ python application.py\n",
    " * Serving Flask app \"application\" (lazy loading)\n",
    " * Environment: production\n",
    "   WARNING: This is a development server. Do not use it in a production deployment.\n",
    "   Use a production WSGI server instead.\n",
    " * Debug mode: off\n",
    " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "The last line of the output tells you that this is running at port 5000.\n",
    "In a separate terminal you can test the API by running\n",
    "```bash\n",
    "$ curl 'localhost:5000/?sepal_length=7.7&sepal_width=3.0&petal_length=6.1&petal_width=2.3'\n",
    "{\"prediction\":\"2\"}\n",
    "```\n",
    "(You can also replace `localhost` with `0.0.0.0`.)\n",
    "\n",
    "And voil√†! Your model is now available via an API... even if it's only locally.\n",
    "This delivery mechanism could be achieved by sending the files `application.py`\n",
    "and `model.pickle` to whoever wants to run this application, assuming they have\n",
    "the ability to receive incoming requests and the proper versions of python\n",
    "and all its packages installed... We'll get to the environment problem soon, but\n",
    "first, let's speed things up.\n",
    "\n",
    "### Gunicorn\n",
    "\n",
    "When we ran our Flask application, we helpfully received the message\n",
    "```bash\n",
    "WARNING: This is a development server. Do not use it in a production deployment.\n",
    "```\n",
    "The reason for this warning is that Flask is slow. The web server that Flask\n",
    "creates works by sitting there waiting\n",
    "for requests which it tries to execute one at a time. This can cause requests to back\n",
    "up and might cause errors. It's fine for development, but is not sufficient for \n",
    "production.\n",
    "\n",
    "A quick way to speed things up is to pair Flask with the python package\n",
    "[`gunicorn`](https://gunicorn.org/) aka Green Unicorn. (Unicorn is a web server \n",
    "for ruby applications, pythons are green I suppose so... green unicorn.) Gunicorn will\n",
    "sit in front of the Flask web server and create multiple workers. When a request\n",
    "comes it, it will be routed to one fo the workers that are hopefully idle.\n",
    "\n",
    "In order to use gunicorn, we will add the following file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wsgi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wsgi.py\n",
    "\n",
    "from application import app\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(use_reloader=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acronym WSGI stands for [_Web Server Gateway Interface_](https://wsgi.readthedocs.io/en/latest/)\n",
    "which is a Python standard specification for how web servers and web applications communicate.\n",
    "Gunicorn is a WSGI server and will use this file to load `app` from the `application` module\n",
    "that we previously wrote. With just this bit of code, we can now start a webserver with\n",
    "gunicorn with the command\n",
    "```bash\n",
    "$ gunicorn -w 3 -b :5001 -t 360 --reload wsgi:app\n",
    "[2020-06-28 15:40:12 -0400] [86149] [INFO] Starting gunicorn 20.0.4\n",
    "[2020-06-28 15:40:12 -0400] [86149] [INFO] Listening at: http://0.0.0.0:5001 (86149)\n",
    "[2020-06-28 15:40:12 -0400] [86149] [INFO] Using worker: sync\n",
    "[2020-06-28 15:40:12 -0400] [86152] [INFO] Booting worker with pid: 86152\n",
    "[2020-06-28 15:40:12 -0400] [86153] [INFO] Booting worker with pid: 86153\n",
    "[2020-06-28 15:40:12 -0400] [86154] [INFO] Booting worker with pid: 86154\n",
    "```\n",
    "You can tell that we have started three workers and we are listening at\n",
    "port 5001 of `localhost`. We can again cURL to get a response:\n",
    "```bash\n",
    "$ curl 'localhost:5001/?sepal_length=7.7&sepal_width=3.0&petal_length=6.1&petal_width=2.3'\n",
    "{\"prediction\":\"2\"}\n",
    "```\n",
    "Note that we did not have to curl a specific worker, we just had to \n",
    "curl to the port and gunicorn took care of delgating the task for us.\n",
    "\n",
    "### Nginx?\n",
    "\n",
    "The gunicorn documentation will also tell you not to deploy an application just\n",
    "with gunicorn and will point you to nginx. We won't use the nginx webserver here\n",
    "(apache is another popular one), but we'll remark that nginx sits in front of\n",
    "gunicorn in a similar way to how gunicorn sits in front of flask. Nginx\n",
    "should be used to actually accept traffic from the web and will direct\n",
    "requests to appropriate places and serve files directly to users if needed. Nginx\n",
    "is an all purpose web server, while gunicorn just serves python applications.\n",
    "\n",
    "\n",
    "\n",
    "## Standardizing environments\n",
    "\n",
    "\n",
    "\n",
    "A problem which has long plagued software engineers and now also data scientists\n",
    "is the fact that environments are not standardized. One data scientist will not\n",
    "necessarily have the same exact version of `pandas` installed. Or even the same\n",
    "verison of `python`. Or even the same operating system. How do we solve for this?\n",
    "\n",
    "### Local environments\n",
    "\n",
    "One way to make sure that everyone is using the same packages and python\n",
    "versions is to use the `venv` module. Going this route will create directories\n",
    "in the directory where venv is called and installed python packages will live there.\n",
    "At least among data scientists, this has fallen out of fashion. More information\n",
    "can be found [here](https://docs.python.org/3/library/venv.html).\n",
    "\n",
    "Since most python based data scientists use Anaconda, it is convenient to rely\n",
    "on the environment abilities of the `conda` tool, which is able to create\n",
    "environments from environment files that can be checked into `git` repositories.\n",
    "For example, here is the contents of the `environment.yml` file that I am using:\n",
    "```yaml\n",
    "name: erdos\n",
    "dependencies:\n",
    "    - python=3.8\n",
    "    - pandas=1.0.*\n",
    "    - numpy=1.18.*\n",
    "    - ipykernel=5.1.*\n",
    "    - jupyterlab=1.2.*\n",
    "    - scikit-learn=0.22.1\n",
    "    - nb_conda_kernels=2.2.*\n",
    "    - flask\n",
    "    - gunicorn\n",
    "    - pip\n",
    "```\n",
    "This is a YAML file, where YAML which stands for YAML Ain't Markup Language. YAML\n",
    "files are just text files with either the `.yml` or `.yaml` endings which are\n",
    "increasingly common in software engineering in various \"__ as code\" situations.\n",
    "You can create a new conda environment and make it available to jupyter\n",
    "by running\n",
    "```bash\n",
    "$ conda env create -f environment.yml\n",
    "$ python -m ipykernel install --user --name erdos --display-name \"Python (erdos)\"\n",
    "```\n",
    "Updates are handled via `conda env update -f environment.yml`.\n",
    "\n",
    "\n",
    "### Production environments\n",
    "\n",
    "\n",
    "While conda is fine for maintaing local environments among teammates working\n",
    "on a project, production servers will not have anaconda installed and may not\n",
    "even have the same operating system as personal development laptops. I.e.,\n",
    "you might have mac OS or an ubuntu on your machine, but the production servers might\n",
    "run CentOS.\n",
    "\n",
    "A decade ago the answer to the operating system issue would be virtual machines. \n",
    "A virtual machine allows you to run a full copy of one operating system on another.\n",
    "The production cluster where applications are deployed might contain servers running \n",
    "CentOS or Red Hat Enterprise Linux or whatever operating system,\n",
    "but these would just be hosts that run virtual machines that can then be any\n",
    "other operating system. If your application required a flavor of linux you \n",
    "would deploy to the appropriate virtual machine.\n",
    "\n",
    "This still does not solve the package and versioning issues. The technology \n",
    "that developed to solve both issues at once is called _Docker_. [Docker](https://www.docker.com/)\n",
    "bundles together basic operating system information and application information into\n",
    "an _image_. Without getting into \n",
    "technical details, it suffices to know that docker images are much more lightweight\n",
    "than full virtual machines even though they naively seem very similar. Docker\n",
    "images define what are called _microservices_; where a virtual machine can run many\n",
    "applications at the same time, a docker image should only handle small\n",
    "tasks and will not have many applications running at once.\n",
    "\n",
    "If you have docker installed, here's how to see which images you have locally and to run `bash` inside\n",
    " of an ubuntu image.\n",
    "```bash\n",
    "$ docker images\n",
    "$ docker run --rm -it ubuntu:18.04 /bin/bash\n",
    "```\n",
    "\n",
    "If you don't have this version of the image locally, the required images will \n",
    "be automatically pulled from the [default docker registry](https://hub.docker.com/_/ubuntu).\n",
    "Playing around inside you can see that basic things like `python` are not \n",
    "installed, but it is a version of the ubuntu operating system. It's also much\n",
    "smaller than an ubuntu VM, being only 64 MB.\n",
    "A running instance of an image is running it is called a _container_, such that\n",
    "you can have many containers running from the same image. You can see\n",
    "the running containers with the command `docker ps` and see all stopped\n",
    "containers alon with the running containers with `docker ps -a`. Here is the\n",
    "[docker cheat sheet](https://www.docker.com/sites/default/files/d8/2019-09/docker-cheat-sheet.pdf).\n",
    "\n",
    "With that introduction out of the way, let's get to an example of how to build\n",
    "a docker image and package up our gunicorn + flask based API.\n",
    "\n",
    "The basic unit that defines how an image is built is a  _Dockerfile_. We will walk \n",
    "through the following Dockerfile to see how one might build an image to deploy\n",
    "our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "FROM python:3.8\n",
    "\n",
    "# system updates\n",
    "RUN apt-get update\n",
    "\n",
    "# create and switch to non-root user\n",
    "RUN groupadd appgroup\n",
    "RUN useradd -g appgroup -m appuser\n",
    "USER appuser\n",
    "WORKDIR /home/appuser/\n",
    "\n",
    "# install requirements\n",
    "COPY requirements.txt .\n",
    "ENV PATH /home/appuser/.local/bin:$PATH\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt --user\n",
    "\n",
    "# build the model\n",
    "COPY build_model.py .\n",
    "RUN python build_model.py\n",
    "\n",
    "# start it up\n",
    "COPY application.py .\n",
    "COPY wsgi.py .\n",
    "CMD gunicorn -w 3 -b :5000 -t 360 --reload wsgi:app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's walkthrough this to get a feel for the kinds of things that we can do when building\n",
    "a docker image.\n",
    "\n",
    "We start off with `FROM python:3.8` which specifies that we want to start with the base\n",
    "image that is called `python` and is tagged with version `3.8`. This base image exists\n",
    "in the default [docker registry](https://hub.docker.com/_/python). Your company will have its\n",
    "own privately hosted registry that you might pull images from and push images to (similar to\n",
    "pulling and pushing from a git repository). Docker images are built in layers and\n",
    "this tells it which image to start building from.\n",
    "The python image uses a Debian based (aka Ubuntu-like) operating system.\n",
    "\n",
    "When we ran the Ubuntu image earlier, you might have noticed that we were running as\n",
    "the root user with full privileges. This is not usually recommended for security reasons,\n",
    "so the first thing we'll do is create a new group and user that we can run as. This is\n",
    "contained in the lines\n",
    "```text\n",
    "# create and switch to non-root user\n",
    "RUN groupadd appgroup\n",
    "RUN useradd -g appgroup -m appuser\n",
    "USER appuser\n",
    "WORKDIR /home/appuser/\n",
    "```\n",
    "\n",
    "Note that if we have a `RUN` command, then that command is run inside the image that\n",
    "we are building, not locally. This is important to keep in mind, because if you want to\n",
    "use `RUN` to call a piece of code, that code needs to exist inside the image.\n",
    "The `USER` command switches us to that user. The\n",
    "`WORKDIR` command switches which directory we are in. \n",
    "\n",
    "The next thing we do is we set up the python packages that we want. We will\n",
    "put our requirements not in a YAML file, but in a plain text file that\n",
    "we can install from using pip. \n",
    "```text\n",
    "COPY requirements.txt .\n",
    "ENV PATH /home/appuser/.local/bin:$PATH\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip install --no-cache-dir -r requirements.txt --user\n",
    "```\n",
    "\n",
    "We can see that we used the `COPY` command to copy the requirements\n",
    "file from our local directory into the image. We then append a local\n",
    "directory to our path (this was necessary to call gunicorn), upgrade `pip`,\n",
    "and finally install requirements from the file that we loaded\n",
    "into the image.\n",
    "\n",
    "\n",
    "The next bit of code copies a new module called `build_model.py` into \n",
    "the image and runs it inside the image to build our model and save it\n",
    "into the image. We'll see this module in a minute.\n",
    "```text\n",
    "# build the model\n",
    "COPY build_model.py .\n",
    "RUN python build_model.py\n",
    "```\n",
    "\n",
    "Recall that we built the model using a dataset that came loaded into\n",
    "`scikit-learn`. Normally you will have to configure your image to\n",
    "pull data from one of your company's databases. This might involve\n",
    "somehow passing your credentials into the image so that the image has\n",
    "the right to access the data or if the images are being built automatically\n",
    "on some other server, more likely you'll have to pass in some other set of\n",
    "credentials. A fun problem to solve is to write an image that you can\n",
    "build locally with your own credentials for testing and also build using\n",
    "whatever system your company uses to automatically build images.\n",
    "\n",
    "Next we copy in the `application.py` and `wsgi.py` modules that we\n",
    "wrote earlier. \n",
    "```text\n",
    "# start it up\n",
    "COPY application.py .\n",
    "COPY wsgi.py .\n",
    "CMD gunicorn -w 3 -b :5000 -t 360 --reload wsgi:app\n",
    "```\n",
    "\n",
    "The final line contains the `CMD` that is used by default  when\n",
    "starting a container frmo this image.\n",
    "(Earlier when we started the ubuntu container, we passed in `/bin/bash` which would \n",
    "overwrite this command.)\n",
    "\n",
    "Below this are the `build_model.py` and `requirements.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_model.py\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "# load dataset\n",
    "iris = datasets.load_iris()\n",
    "df_iris = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "df_iris['target'] = iris['target']\n",
    "\n",
    "# create train test split\n",
    "df_train, df_test = train_test_split(df_iris,\n",
    "                                     test_size=0.3,\n",
    "                                     random_state=47,\n",
    "                                     stratify=df_iris['target'])\n",
    "\n",
    "# put together pipeline\n",
    "model = Pipeline([\n",
    "    ('scale', StandardScaler())\n",
    "    ,('gbm', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# make model\n",
    "model.fit(df_train[iris['feature_names']], df_train['target'])\n",
    "\n",
    "# see how accurate it is on test data\n",
    "test_predictions = model.predict(df_test[iris['feature_names']])\n",
    "test_accuracy = accuracy_score(df_test['target'], test_predictions)\n",
    "print('Test accuracy of fit model: {:.3f}'.format(test_accuracy))\n",
    "\n",
    "# save model\n",
    "pickle.dump(model, open('./model.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "scikit-learn==0.22.1\n",
    "pandas==1.0.5\n",
    "flask==1.1.2\n",
    "gunicorn==20.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's examine what happens when we build this image.\n",
    "To build an image and `tag` it with the name `iris_api` we run\n",
    "the following:\n",
    "```bash\n",
    "$ docker build . -t iris_api\n",
    "```\n",
    "\n",
    "Examaning the output of this command, you can see it building\n",
    "the image in layers. If you change one part of the Dockerfile,\n",
    "docker might not have to rebuild the parts above your image (although\n",
    "this is not entirely true... note that it will always need to install\n",
    "the requirements...).\n",
    "As we did with the Ubuntu container, let's start a container\n",
    "from this image. Instead of running the default `CMD` above,\n",
    "let's get into the image.\n",
    "```bash\n",
    "$ docker run --rm -it iris_api /bin/bash\n",
    "```\n",
    "\n",
    "Note that we were in the `WORKDIR` that we specified earlier\n",
    "and that we were the `appuser` user and not `root`. To start\n",
    "a container with the default command, we can run.\n",
    "```bash\n",
    "$ docker run --rm -p 8747:5000 --name iris_api_1 iris_api\n",
    "```\n",
    "\n",
    "Recall that the `CMD` we specified for the image ran gunicorn on port `5000`.\n",
    "In this `docker run` command we use the `-p 8747:5000` option to attach localhost's\n",
    "port `8747` to the containers port `5000`. In another terminal window, we\n",
    "can curl to this port on localhost to get a result.\n",
    "```bash\n",
    "$ curl 'localhost:8747/?sepal_length=7.7&sepal_width=3.0&petal_length=6.1&petal_width=2.3'\n",
    "```\n",
    "\n",
    "And now if you want to deliver your model to someone, all you need to care about\n",
    "is that they have docker installed. The operating system doesn't matter and they\n",
    "don't even need to have python installed! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "- Investigate how you can set environment variables inside your image using `ENV` and `ARG`\n",
    "commands. What is the difference between them?\n",
    "- Instead of copying a file into the image, try _mounting_ a file as a volume\n",
    "into a container, writing to it from inside the container, \n",
    "and confirming that the local file persists with that data after the container is gone.\n",
    "- It is often necessary to pass in secrets such as username + password combinations when\n",
    "building an image. Look into enabling the _buildkit_ and how to use the `--secret` flag\n",
    "so that passwords are not saved in intermediate images.\n",
    "- Create your own private registry by running a registry inside a container on \n",
    "your local machine. Tag an image that exists locally, push it to the registry, \n",
    "remove the local image, and pull down the image from the registry container.\n",
    "- Build your model in one image, then create a second image based on the first image\n",
    "(similar to how our image was based on the `python:3.8` image) to serve the model.\n",
    "- Use [docker compose](https://docs.docker.com/compose/) to create an application that has\n",
    "multiple containers. Have one container run nginx (these containers exist already!) and \n",
    "have one run your model application. Curl requests to the nginx container instead of the\n",
    "model application container.\n",
    "- Put the iris data into a database (such as postgres) running in one image and have your model pull\n",
    "the data from that image (you will need to create a user that your image can use to log in\n",
    "as!).\n",
    "- Investigate [Kubernetes](https://kubernetes.io/), aka k8s, which can be used to run\n",
    "many images.\n",
    "- Try running your docker image on a virtual machine in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post was created for a lecture for the [Erdos institute](erdosinstitute.org)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (erdos)",
   "language": "python",
   "name": "erdos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
